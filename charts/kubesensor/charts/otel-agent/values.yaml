# Default values for otel-agent.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

global:
  cluster_name: k8s-d-rMZzQiOarI
  kubeOtelIp: kubesense-kubeotel
  kubeOtelGrpcPort: 30050
  vmAgentIp: kubesense-victoria-metrics-agent
  vmAgentHttpPort: 30060

nameOverride: ""
fullnameOverride: ""

# Valid values are "daemonset", "deployment", and "statefulset".
mode: "deployment"

# Specify which namespace should be used to deploy the resources into
namespaceOverride: ""

# Handles basic configuration of components that
# also require k8s modifications to work correctly.
# .Values.config can be used to modify/add to a preset
# component configuration, but CANNOT be used to remove
# preset configuration. If you require removal of any
# sections of a preset configuration, you cannot use
# the preset. Instead, configure the component manually in
# .Values.config and use the other fields supplied in the
# values.yaml to configure k8s as necessary.
presets:
  # Configures the collector to collect logs.
  # Adds the filelog receiver to the logs pipeline
  # and adds the necessary volumes and volume mounts.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#filelog-receiver for details on the receiver.
  logsCollection:
    enabled: false
    includeCollectorLogs: false
    # Enabling this writes checkpoints in /var/lib/otelcol/ host directory.
    # Note this changes collector's user to root, so that it can write to host directory.
    storeCheckpoints: false
    # The maximum bytes size of the recombined field.
    # Once the size exceeds the limit, all received entries of the source will be combined and flushed.
    maxRecombineLogSize: 102400
  # Configures the collector to collect host metrics.
  # Adds the hostmetrics receiver to the metrics pipeline
  # and adds the necessary volumes and volume mounts.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#host-metrics-receiver for details on the receiver.
  hostMetrics:
    enabled: false
  # Configures the Kubernetes Processor to add Kubernetes metadata.
  # Adds the k8sattributes processor to all the pipelines
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-attributes-processor for details on the receiver.
  kubernetesAttributes:
    enabled: false
    # When enabled the processor will extra all labels for an associated pod and add them as resource attributes.
    # The label's exact name will be the key.
    extractAllPodLabels: false
    # When enabled the processor will extra all annotations for an associated pod and add them as resource attributes.
    # The annotation's exact name will be the key.
    extractAllPodAnnotations: false
  # Configures the collector to collect node, pod, and container metrics from the API server on a kubelet..
  # Adds the kubeletstats receiver to the metrics pipeline
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = daemonset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubeletstats-receiver for details on the receiver.
  kubeletMetrics:
    enabled: false
  # Configures the collector to collect kubernetes events.
  # Adds the k8sobject receiver to the logs pipeline
  # and collects kubernetes events by default.
  # Best used with mode = deployment or statefulset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-objects-receiver for details on the receiver.
  kubernetesEvents:
    enabled: false
  # Configures the Kubernetes Cluster Receiver to collect cluster-level metrics.
  # Adds the k8s_cluster receiver to the metrics pipeline
  # and adds the necessary rules to ClusteRole.
  # Best used with mode = deployment or statefulset.
  # See https://opentelemetry.io/docs/kubernetes/collector/components/#kubernetes-cluster-receiver for details on the receiver.
  clusterMetrics:
    enabled: false

configMap:
  # Specifies whether a configMap should be created (true by default)
  create: true


# Base collector configuration.
# Supports templating. To escape existing instances of {{ }}, use {{` <original content> `}}.
# For example, {{ REDACTED_EMAIL }} becomes {{` {{ REDACTED_EMAIL }} `}}.
config:
  extensions:
    # The health_check extension is mandatory for this chart.
    # Without the health_check extension the collector will fail the readiness and liveliness probes.
    # The health_check extension can be modified, but should never be removed.
    health_check:
      endpoint: ${env:MY_POD_IP}:13133
    pprof:
      endpoint: :1888
    zpages:
      endpoint: :55679
  receivers:
    kubesensek8sevents:
      cluster: k8s-d-rMZzQiOarI
      entities:
        - name: events
          mode: watch
    kubeletstats_2:
      collection_interval: 10s
      auth_type: "serviceAccount"
      endpoint: https://10.128.15.208:10250
      insecure_skip_verify: true
      metric_groups:
      - container
      - pod
      - volume
      - node
      extra_metadata_labels:
        - container.id
      k8s_api_config:
        auth_type: serviceAccount 

    k8s_cluster:
      collection_interval: 10s
      node_conditions_to_report: [Ready, MemoryPressure,DiskPressure,NetworkUnavailable]
      allocatable_types_to_report: [cpu, memory, storage, ephemeral-storage]
      auth_type: serviceAccount

    prometheus:
      config:
        scrape_configs:
          - job_name: ksm-metrics
            scrape_interval: 10s
            metrics_path: /metrics
            static_configs:
              - targets: ["kubesense-kube-state-metrics:8080"]
            
            metric_relabel_configs:
              # First block adds ckubesense prefix to all the label 
              - action: replace
                source_labels: [__name__]
                regex: '(.*)'
                target_label: __name__
                replacement: kubesense_$${1}

                
              # Second block filters out resource limits and tag cpu and memory
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_pod_container_resource_limits;cpu'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_cpu_limit_m_cpu'
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_pod_container_resource_limits;memory'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_memory_limit_bytes'      


              # Third block filters out resource request and tag cpu and memory
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_pod_container_resource_requests;cpu'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_cpu_request_m_cpu'
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_pod_container_resource_requests;memory'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_memory_request_bytes' 

              # container cpu usage seconds total  
              - source_labels: [__name__]
                regex: 'container_cpu_usage_seconds_total'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_m_cpu_usage_seconds_total'
              - source_labels: [__name__]
                regex: 'container_memory_working_set_bytes'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_mem_working_set_bytes'  
              - source_labels: [__name__]
                regex: 'container_cpu_cfs_throttled_seconds_total'
                action: replace
                target_label: __name__
                replacement: 'kubesense_container_cpu_throttled_seconds_total'

              # Fourth block filters out resource node cpu and memory
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_node_status_allocatable;cpu'
                action: replace
                target_label: __name__
                replacement: 'kubesense_node_allocatable_cpum_cpu'
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_node_status_allocatable;memory'
                action: replace
                target_label: __name__
                replacement: 'kubesense_node_allocatable_mem_bytes'

              # Fifth block filters out resource node cpu and memory
              - source_labels: [__name__,resource]
                regex: 'kubesense_kube_node_status_capacity;ephemeral_storage'
                action: replace
                target_label: __name__
                replacement: 'kubesense_node_disk_space_total_bytes'
              # - source_labels: [__name__,resource]
              #   regex: 'kubesense_kube_node_status_allocatable;memory'
              #   action: replace
              #   target_label: __name__
              #   replacement: 'kubesense_node_allocatable_mem_bytes'  

              # sixth block filters out pvc related metrics
              - source_labels: [__name__]
                regex: 'kubelet_volume_stats_available_bytes'
                action: replace
                target_label: __name__
                replacement: 'kubesense_pvc_available_bytes'
              - source_labels: [__name__]
                regex: 'kubelet_volume_stats_capacity_bytes'
                action: replace
                target_label: __name__
                replacement: 'kubesense_pvc_capacity_bytes'  
              - source_labels: [__name__]
                regex: 'kubelet_volume_stats_used_bytes'
                action: replace
                target_label: __name__
                replacement: 'kubesense_pvc_usage_bytes'    

          #  job 2 scrapes from kube apis
          # - job_name: "prometheus"
          #   scrape_interval: 10s
          #   kubernetes_sd_configs:
          #     - role: pod
          #   relabel_configs:
          #     - source_labels:
          #         [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
          #       regex: "true"
          #       action: keep
          #     - source_labels:
          #         [
          #           __address__,
          #           __meta_kubernetes_pod_annotation_prometheus_io_port,
          #         ]
          #       action: replace
          #       target_label: __address__
          #       regex: ([^:]+)(?::\d+)?;(\d+)
          #       replacement: $$1:$$2

          # Scrape cAdvisor metrics
          - job_name: integrations/kubernetes/cadvisor
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            kubernetes_sd_configs:
              - role: node
            relabel_configs:
              - replacement: kubernetes.default.svc.cluster.local:443
                target_label: __address__
              - regex: (.+)
                replacement: /api/v1/nodes/$${1}/proxy/metrics/cadvisor
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: __metrics_path__
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes

          # Scrape kublet metrics
          - job_name: integrations/kubernetes/kubelet
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            kubernetes_sd_configs:
              - role: node
            relabel_configs:
              - replacement: kubernetes.default.svc.cluster.local:443
                target_label: __address__
              - regex: (.+)
                replacement: /api/v1/nodes/$${1}/proxy/metrics
                source_labels:
                  - __meta_kubernetes_node_name
                target_label: __metrics_path__
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: false
              server_name: kubernetes
          
          # Scrape config for API servers
          - job_name: "kubernetes-apiservers"
            kubernetes_sd_configs:
              - role: endpoints
                namespaces:
                  names:
                    - default
            scheme: https
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
            relabel_configs:
              - source_labels: [__meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
                action: keep
                regex: kubernetes;https
              - action: replace
                source_labels:
                - __meta_kubernetes_namespace
                target_label: Namespace
              - action: replace
                source_labels:
                - __meta_kubernetes_service_name
                target_label: Service  

  exporters:
    otlp:
      # refer _config.yaml for templating this is kept as default values
      endpoint: "{{ .Values.global.kubeAggregatorIp }}:{{ .Values.global.nodePort.kubeAggregatorGrpcPort }}"
      tls:
        insecure: true
    prometheusremotewrite:
      # endpoint: "http://kubesense-victoria-metrics-agent:8429/api/v1/write"
      endpoint: "{{ if $.Values.global.externalVictoriaMetrics.enabled }}{{ $.Values.global.externalVictoriaMetrics.url }}{{ else }}http://{{ .Values.global.vmAgentIp }}:{{ .Values.global.vmAgentHttpPort }}{{ end }}/api/v1/write"
      resource_to_telemetry_conversion:
        enabled: true      
      timeout: 60s
      retry_on_failure:
        enabled: false
  processors:
    batch: {}
    # If set to null, will be overridden with values based on k8s resource limits
    memory_limiter:
      check_interval: 5s
      limit_percentage: 75
      spike_limit_percentage: 10
    # metricstransform:
    #   transforms:
    #     - include: "*"
    #       match_type: regexp
    #       action: update
    #       new_name: "kubesense_${1}"
  service:
    pipelines:
      metrics:
        receivers: [kubeletstats_2,prometheus,k8s_cluster]
        # processors: [metricstransform]
        exporters: [prometheusremotewrite]
      logs:
        receivers: [kubesensek8sevents]
        processors: [batch]
        exporters: [otlp]

image:
  # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
  repository: 365639915496.dkr.ecr.us-east-1.amazonaws.com/kubeotel
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: v1.0.1
  # When digest is set to a non-empty value, images will be pulled by digest (regardless of tag value).
  digest: ""
imagePullSecrets: 
  - name: "registry-credentials"
# imagePullSecrets: []
# OpenTelemetry Collector executable
# command:
#   name: otelcol-contrib
#   extraArgs: []

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

clusterRole:
  # Specifies whether a clusterRole should be created
  # Some presets also trigger the creation of a cluster role and cluster role binding.
  # If using one of those presets, this field is no-op.
  create: false
  # Annotations to add to the clusterRole
  # Can be used in combination with presets that create a cluster role.
  annotations: {}
  # The name of the clusterRole to use.
  # If not set a name is generated using the fullname template
  # Can be used in combination with presets that create a cluster role.
  name: ""
  # A set of rules as documented here : https://kubernetes.io/docs/reference/access-authn-authz/rbac/
  # Can be used in combination with presets that create a cluster role to add additional rules.
  rules: []
  # - apiGroups:
  #   - ''
  #   resources:
  #   - 'pods'
  #   - 'nodes'
  #   verbs:
  #   - 'get'
  #   - 'list'
  #   - 'watch'

  clusterRoleBinding:
    # Annotations to add to the clusterRoleBinding
    # Can be used in combination with presets that create a cluster role binding.
    annotations: {}
    # The name of the clusterRoleBinding to use.
    # If not set a name is generated using the fullname template
    # Can be used in combination with presets that create a cluster role binding.
    name: ""

podSecurityContext: {}
securityContext: {}

nodeSelector: {}
tolerations: []
affinity: {}
topologySpreadConstraints: []

# Allows for pod scheduler prioritisation
priorityClassName: ""

extraEnvs: []
extraEnvsFrom: []
extraVolumes: []
extraVolumeMounts: []

# Configuration for ports
# nodePort is also allowed
ports:
  otlp:
    enabled: true
    containerPort: 4317
    servicePort: 4317
    hostPort: 4317
    protocol: TCP
    # nodePort: 30317
    appProtocol: grpc
  otlp-http:
    enabled: true
    containerPort: 4318
    servicePort: 4318
    hostPort: 4318
    protocol: TCP
  jaeger-compact:
    enabled: true
    containerPort: 6831
    servicePort: 6831
    hostPort: 6831
    protocol: UDP
  jaeger-thrift:
    enabled: true
    containerPort: 14268
    servicePort: 14268
    hostPort: 14268
    protocol: TCP
  jaeger-grpc:
    enabled: true
    containerPort: 14250
    servicePort: 14250
    hostPort: 14250
    protocol: TCP
  zipkin:
    enabled: true
    containerPort: 9411
    servicePort: 9411
    hostPort: 9411
    protocol: TCP
  metrics:
    # The metrics port is disabled by default. However you need to enable the port
    # in order to use the ServiceMonitor (serviceMonitor.enabled) or PodMonitor (podMonitor.enabled).
    enabled: false
    containerPort: 8888
    servicePort: 8888
    protocol: TCP

# Resource limits & requests. Update according to your own use case as these values might be too low for a typical deployment.
resources: {}
# resources:
#   limits:
#     cpu: 250m
#     memory: 512Mi

podAnnotations: {}

podLabels: {}

# Common labels to add to all otel-collector resources. Evaluated as a template.
additionalLabels: {}
#  app.kubernetes.io/part-of: my-app

# Host networking requested for this pod. Use the host's network namespace.
hostNetwork: false

# Adding entries to Pod /etc/hosts with HostAliases
# https://kubernetes.io/docs/tasks/network/customize-hosts-file-for-pods/
hostAliases: []
  # - ip: "1.2.3.4"
  #   hostnames:
  #     - "my.host.com"

# Pod DNS policy ClusterFirst, ClusterFirstWithHostNet, None, Default, None
dnsPolicy: ""

# Custom DNS config. Required when DNS policy is None.
dnsConfig: {}

# only used with deployment mode
replicaCount: 1

# only used with deployment mode
revisionHistoryLimit: 10

annotations: {}

# List of extra sidecars to add
extraContainers: []
# extraContainers:
#   - name: test
#     command:
#       - cp
#     args:
#       - /bin/sleep
#       - /test/sleep
#     image: busybox:latest
#     volumeMounts:
#       - name: test
#         mountPath: /test

# List of init container specs, e.g. for copying a binary to be executed as a lifecycle hook.
# Another usage of init containers is e.g. initializing filesystem permissions to the OTLP Collector user `10001` in case you are using persistence and the volume is producing a permission denied error for the OTLP Collector container.
initContainers: []
# initContainers:
#   - name: test
#     image: busybox:latest
#     command:
#       - cp
#     args:
#       - /bin/sleep
#       - /test/sleep
#     volumeMounts:
#       - name: test
#         mountPath: /test
#  - name: init-fs
#    image: busybox:latest
#    command:
#      - sh
#      - '-c'
#      - 'chown -R 10001: /var/lib/storage/otc' # use the path given as per `extensions.file_storage.directory` & `extraVolumeMounts[x].mountPath`
#    volumeMounts:
#      - name: opentelemetry-collector-data # use the name of the volume used for persistence
#        mountPath: /var/lib/storage/otc # use the path given as per `extensions.file_storage.directory` & `extraVolumeMounts[x].mountPath`

# Pod lifecycle policies.
lifecycleHooks: {}
# lifecycleHooks:
#   preStop:
#     exec:
#       command:
#       - /test/sleep
#       - "5"

# liveness probe configuration
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
##
livenessProbe:
  # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
  # initialDelaySeconds: 1
  # How often in seconds to perform the probe.
  periodSeconds: 15
  # Number of seconds after which the probe times out.
  timeoutSeconds: 1
  # Minimum consecutive failures for the probe to be considered failed after having succeeded.
  # failureThreshold: 1
  # Duration in seconds the pod needs to terminate gracefully upon probe failure.
  # terminationGracePeriodSeconds: 10
  httpGet:
    port: 8888
    path: /metrics

# # readiness probe configuration
# # Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/
# ##
# readinessProbe:
#   # Number of seconds after the container has started before startup, liveness or readiness probes are initiated.
#   # initialDelaySeconds: 1
#   # How often (in seconds) to perform the probe.
#   # periodSeconds: 10
#   # Number of seconds after which the probe times out.
#   # timeoutSeconds: 1
#   # Minimum consecutive successes for the probe to be considered successful after having failed.
#   # successThreshold: 1
#   # Minimum consecutive failures for the probe to be considered failed after having succeeded.
#   # failureThreshold: 1
#   httpGet:
#     port: 13133
#     path: /

service:
  # Enable the creation of a Service.
  # By default, it's enabled on mode != daemonset.
  # However, to enable it on mode = daemonset, its creation must be explicitly enabled
  # enabled: true

  type: ClusterIP
  # type: LoadBalancer
  # loadBalancerIP: 1.2.3.4
  # loadBalancerSourceRanges: []

  # By default, Service of type 'LoadBalancer' will be created setting 'externalTrafficPolicy: Cluster'
  # unless other value is explicitly set.
  # Possible values are Cluster or Local (https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/#preserving-the-client-source-ip)
  # externalTrafficPolicy: Cluster

  annotations: {}

  # By default, Service will be created setting 'internalTrafficPolicy: Local' on mode = daemonset
  # unless other value is explicitly set.
  # Setting 'internalTrafficPolicy: Cluster' on a daemonset is not recommended
  # internalTrafficPolicy: Cluster

ingress:
  enabled: false
  # annotations: {}
  # ingressClassName: nginx
  # hosts:
  #   - host: collector.example.com
  #     paths:
  #       - path: /
  #         pathType: Prefix
  #         port: 4318
  # tls:
  #   - secretName: collector-tls
  #     hosts:
  #       - collector.example.com

  # Additional ingresses - only created if ingress.enabled is true
  # Useful for when differently annotated ingress services are required
  # Each additional ingress needs key "name" set to something unique
  additionalIngresses: []
  # - name: cloudwatch
  #   ingressClassName: nginx
  #   annotations: {}
  #   hosts:
  #     - host: collector.example.com
  #       paths:
  #         - path: /
  #           pathType: Prefix
  #           port: 4318
  #   tls:
  #     - secretName: collector-tls
  #       hosts:
  #         - collector.example.com

podMonitor:
  # The pod monitor by default scrapes the metrics port.
  # The metrics port needs to be enabled as well.
  enabled: false
  metricsEndpoints:
    - port: metrics
      # interval: 15s

  # additional labels for the PodMonitor
  extraLabels: {}
  #   release: kube-prometheus-stack

serviceMonitor:
  # The service monitor by default scrapes the metrics port.
  # The metrics port needs to be enabled as well.
  enabled: false
  metricsEndpoints:
    - port: metrics
      # interval: 15s

  # additional labels for the ServiceMonitor
  extraLabels: {}
  #  release: kube-prometheus-stack

# PodDisruptionBudget is used only if deployment enabled
podDisruptionBudget:
  enabled: false
#   minAvailable: 2
#   maxUnavailable: 1

# autoscaling is used only if mode is "deployment" or "statefulset"
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  behavior: {}
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

rollout:
  rollingUpdate: {}
  # When 'mode: daemonset', maxSurge cannot be used when hostPort is set for any of the ports
  # maxSurge: 25%
  # maxUnavailable: 0
  strategy: Recreate

prometheusRule:
  enabled: false
  groups: []
  # Create default rules for monitoring the collector
  defaultRules:
    enabled: false

  # additional labels for the PrometheusRule
  extraLabels: {}

statefulset:
  # volumeClaimTemplates for a statefulset
  volumeClaimTemplates: []
  podManagementPolicy: "Parallel"

networkPolicy:
  enabled: false

  # Annotations to add to the NetworkPolicy
  annotations: {}

  # Configure the 'from' clause of the NetworkPolicy.
  # By default this will restrict traffic to ports enabled for the Collector. If
  # you wish to further restrict traffic to other hosts or specific namespaces,
  # see the standard NetworkPolicy 'spec.ingress.from' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  allowIngressFrom: []
  # # Allow traffic from any pod in any namespace, but not external hosts
  # - namespaceSelector: {}
  # # Allow external access from a specific cidr block
  # - ipBlock:
  #     cidr: 192.168.1.64/32
  # # Allow access from pods in specific namespaces
  # - namespaceSelector:
  #     matchExpressions:
  #       - key: kubernetes.io/metadata.name
  #         operator: In
  #         values:
  #           - "cats"
  #           - "dogs"

  # Add additional ingress rules to specific ports
  # Useful to allow external hosts/services to access specific ports
  # An example is allowing an external prometheus server to scrape metrics
  #
  # See the standard NetworkPolicy 'spec.ingress' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  extraIngressRules: []
  # - ports:
  #   - port: metrics
  #     protocol: TCP
  #   from:
  #     - ipBlock:
  #         cidr: 192.168.1.64/32

  # Restrict egress traffic from the OpenTelemetry collector pod
  # See the standard NetworkPolicy 'spec.egress' definition for more info:
  # https://kubernetes.io/docs/reference/kubernetes-api/policy-resources/network-policy-v1/
  egressRules: []
  #  - to:
  #      - namespaceSelector: {}
  #      - ipBlock:
  #          cidr: 192.168.10.10/24
  #    ports:
  #      - port: 1234
  #        protocol: TCP

# When enabled, the chart will set the GOMEMLIMIT env var to 80% of the configured
# resources.limits.memory and remove the memory ballast extension.
# If no resources.limits.memory are defined enabling does nothing.
# In a future release this setting will be enabled by default.
# See https://github.com/open-telemetry/opentelemetry-helm-charts/issues/891
# for more details.
useGOMEMLIMIT: true
